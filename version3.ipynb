{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ddbaf7c-98bd-4f7e-8b36-aee609923808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee53155-05ce-4848-9e5c-c2e8770bda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replace with your path to the Proj2_Data folder on GoogleColab\n",
    "path_to_data='C:/Users/zhaij005/Desktop/uiuc/cs598-Practical Statistical Learning/project 2/Proj2_Data'\n",
    "DATA_DIR ='C:/Users/zhaij005/Desktop/uiuc/cs598-Practical Statistical Learning/project 2/Proj2_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90d985c7-27f9-4854-83ca-0aef0a5ad5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=f'{DATA_DIR}/colab_log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "569976af-c8e1-497a-b9db-d0e4a3acc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "067512a5-f904-4ff4-9f3d-8c90c2ee4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def preprocess(data):\n",
    "    \"\"\"Preprocess the data by handling missing values and creating time-based features.\"\"\"\n",
    "    data.fillna(0, inplace=True)\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Yr2'] = data.Yr ** 2\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])\n",
    "    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d317c315-461f-4b4f-bdd6-29960c3d2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mark holiday weeks\n",
    "def add_holiday_flags(df):\n",
    "    df['Is_SuperBowl'] = (df['Wk'] == 6).astype(int)\n",
    "    df['Is_Thanksgiving'] = (df['Wk'] == 47).astype(int)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    #### try to assign weight of sales based on understanding of how christmas season works ################\n",
    "    conditions = [\n",
    "            ((df['Yr'] == 2010) & (df['Wk'] == 51)),\n",
    "            ((df['Yr'] == 2010) & (df['Wk'] == 52)),\n",
    "            ((df['Yr'] == 2011) & (df['Wk'] == 51)),\n",
    "            ((df['Yr'] == 2011) & (df['Wk'] == 52))\n",
    "        ]\n",
    "    choices = [5, 2, 3, 3]\n",
    "    df['Is_Christmas'] = np.select(conditions, choices, default=0)\n",
    "    ########################################################################################################\n",
    "    df['Is_Pre_ChristmasEve'] = (\n",
    "         ((df['Date'] >= pd.to_datetime('2010-12-23')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2010-12-23')))\n",
    "        |((df['Date'] >= pd.to_datetime('2011-12-23')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2011-12-23')))\n",
    "        |((df['Date'] >= pd.to_datetime('2012-12-23')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2012-12-23')))\n",
    "                                ).astype(int)\n",
    "    df['Is_After_Christmas'] = (\n",
    "         ((df['Date'] >= pd.to_datetime('2010-12-26')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2010-12-26')))\n",
    "        |((df['Date'] >= pd.to_datetime('2011-12-26')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2011-12-26')))\n",
    "        |((df['Date'] >= pd.to_datetime('2012-12-26')) & (df['Date'] - pd.Timedelta(days=7) < pd.to_datetime('2012-12-26')))\n",
    "                                ).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79fe5aff-6212-4c73-8b30-d271f04fa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjustment for dept sales tendency based on history\n",
    "dept_adj = {\n",
    "'Dept':[1,2,3,4,5,6,7,8,9,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,40,41,42,44,45,46,47,48,49,51,52,54,55,56,58,59,60,65,67,71,72,74,79,81,82,83,85,87,91,92,94,96,98,99,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,41,44,45,46,47,48,49,50,51,52,54,55,56,58,59,60,65,71,72,74,81,82,83,85,87,90,91,93,94,95,96,97,98,99,1,2,3,4,5,6,7,8,9,10,11,12,16,18,19,20,21,22,23,24,25,26,27,28,29,30,32,33,34,35,36,37,38,41,42,44,45,47,48,49,50,51,52,54,55,56,58,59,60,65,67,71,72,74,78,79,80,81,82,83,85,87,90,91,92,94,95,96,97,98,99,1,2,3,4,5,6,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25,27,28,29,30,31,32,33,34,35,36,37,38,40,41,42,44,45,46,47,48,50,51,52,54,55,56,58,65,67,71,72,74,78,79,80,81,82,83,85,90,91,92,93,94,95,96,97,98,99,1,2,3,4,5,6,7,9,10,11,12,13,14,16,17,18,19,20,21,24,25,26,27,29,30,31,32,34,35,36,37,38,41,42,44,45,46,47,48,49,50,51,52,54,55,56,58,59,60,65,67,71,72,74,78,79,80,81,82,85,87,90,91,92,93,95,96,97,98,99,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,33,34,35,36,38,40,41,42,44,45,46,47,48,49,50,51,54,56,58,59,60,65,67,71,72,74,77,78,79,80,82,83,85,87,91,92,94,95,96,97,99,1,2,3,4,5,6,7,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,31,32,33,35,36,37,40,41,42,43,44,45,46,47,48,49,50,51,52,54,55,56,58,59,60,65,67,71,72,74,78,79,80,82,83,85,87,90,91,92,93,94,96,97,98,99,1,2,3,4,5,6,7,9,11,12,13,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,41,42,44,45,46,47,48,49,51,52,54,55,56,58,59,60,65,67,71,72,74,77,78,79,80,81,82,83,85,87,90,92,94,95,96,97,98,99,1,2,3,4,6,7,8,9,10,11,12,16,17,18,19,21,22,23,24,25,27,28,30,31,32,33,34,35,36,37,38,40,41,42,44,45,46,47,48,50,51,52,54,55,56,58,59,60,65,67,71,72,74,77,78,79,80,81,82,83,85,87,90,92,93,95,97,99,1,2,3,4,5,6,7,8,10,11,12,14,16,17,18,19,20,21,23,24,25,26,27,28,30,31,32,33,34,35,36,38,40,41,42,44,45,46,47,48,49,50,51,52,54,55,56,58,59,60,65,71,72,74,77,78,79,80,82,83,85,87,90,92,93,94,95,97,98,99],\n",
    "'folder':[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],\n",
    "'adj': [0.94,1.03,0.84,0.98,0.97,1.09,1.29,0.98,1.36,0.9,1.18,0.99,0.98,1.06,0.96,0.37,0.91,1.05,0.99,0.94,1.05,1.02,1.1,1.08,0.85,0.93,0.91,1.11,0.95,0.94,0.97,1.06,1.21,1.3,0.97,0.97,0.97,0.86,1.03,0.97,0.48,1.07,0.1,0.84,1.06,0.13,1.02,0.29,0.98,1.04,0.83,0.66,0.93,1.06,1.09,0.94,0.91,0.95,0.99,0.99,1.01,0.94,0.97,1.03,0.97,0.97,1.05,1.02,0.99,1.2,0.9,0.98,1.06,1.01,1.07,0.89,0.98,1.04,0.98,1.02,0.95,1.01,0.97,1.01,1.03,0.1,0.91,0.94,0.97,0.96,0.95,1.02,1.01,1.07,1.01,1.11,0.93,0.9,1.04,0.98,0.95,0.94,0.91,0.95,0.8,0.96,0.54,1.01,0.1,0.78,0.97,0.98,0.14,0.98,0.26,0.93,0.94,0.66,0.8,0.92,0.9,0.94,0.96,0.95,0.99,0.96,0.93,0.96,0.96,1.01,0.98,0.98,0.97,0.97,0.95,0.99,0.98,0.57,0.99,0.99,1.02,0.99,1.07,0.88,0.99,1.02,1.05,1.03,0.98,1.01,1.13,0.1,0.84,1.03,0.98,0.95,0.98,1.04,1.06,1.04,0.93,1.1,0.95,0.97,1.01,1.02,1.12,0.98,0.94,0.96,1.01,0.85,1.01,0.99,0.33,0.1,0.64,0.98,0.91,0.24,1.11,0.17,0.94,0.89,0.71,0.96,0.95,0.78,0.99,0.97,0.94,0.95,0.83,0.99,1.02,1.01,1.01,0.94,0.97,0.96,1.02,1.01,1.03,1.02,1.03,1.05,0.99,1.01,2.37,0.98,1.02,1.05,1.02,1.21,1.06,1.01,1.01,1.02,1.01,1.04,1.01,1.03,1.11,1.01,0.93,0.82,1.08,1.1,0.99,1.02,1.12,1.05,1.06,0.94,1.02,1.04,0.9,0.96,1.12,1.06,0.98,0.94,1.08,0.99,1.01,0.8,1.05,1.02,0.65,0.97,0.1,0.94,0.95,0.43,1.04,0.1,1.22,0.89,0.92,0.79,1.05,1.09,1.03,1.04,0.1,1.04,1.03,1.02,0.97,1.03,0.97,1.05,1.03,1.05,1.01,1.04,1.03,1.05,0.99,1.01,0.83,1.07,1.06,0.99,1.02,0.95,0.84,0.98,1.11,1.02,0.99,1.05,1.02,1.03,1.01,1.09,1.03,0.88,1.01,1.13,0.99,1.02,0.95,1.09,1.07,1.02,0.88,0.99,1.01,0.95,0.78,1.06,1.02,0.88,1.06,1.08,0.94,1.02,0.1,1.24,0.98,0.98,0.19,1.21,0.1,1.03,0.86,1.08,0.47,0.95,0.78,1.1,1.13,0.99,1.06,0.1,1.07,0.98,0.99,1.01,1.15,1.05,1.02,0.99,1.05,1.01,1.02,0.97,0.97,1.02,3,0.96,1.04,1.02,1.03,1.03,1.2,0.94,1.01,1.06,0.96,1.05,1.14,1.05,1.02,1.2,1.07,0.76,0.99,1.12,1.03,1.07,1.01,1.05,1.1,0.98,1.09,0.98,1.02,0.85,0.93,1.01,1.15,1.04,1.09,0.99,1.04,0.57,1.04,1.06,0.23,0.98,0.1,0.8,1.01,0.95,0.61,0.1,0.94,0.97,0.1,0.94,1.08,0.95,1.07,1.06,1.06,0.97,0.58,1.05,1.01,0.89,1.05,1.02,1.02,1.01,1.03,1.01,1.02,1.02,1.01,0.46,0.93,0.96,0.9,0.99,0.86,0.85,0.93,1.01,1.06,0.99,0.94,0.99,0.96,0.98,0.93,0.52,1.07,0.93,1.04,0.96,1.03,0.97,0.99,0.98,0.97,0.87,0.99,0.97,0.98,0.96,0.88,0.77,0.98,1.01,0.62,0.96,2,0.88,0.45,0.98,0.1,0.95,0.93,0.96,0.16,0.84,0.22,0.83,0.9,0.76,0.1,0.92,1.33,0.94,0.87,0.9,0.95,0.1,0.95,0.97,1.01,0.97,0.91,0.94,0.95,0.99,0.97,0.98,0.93,0.95,0.99,0.96,0.1,0.97,0.97,0.97,0.99,1.05,0.86,0.98,0.95,0.97,0.97,0.99,1.02,0.94,0.98,0.1,1.05,0.92,0.91,0.96,0.97,0.96,0.95,0.93,0.93,0.97,0.97,0.95,1.05,0.93,0.95,0.99,0.86,0.99,0.96,0.72,0.96,1.03,0.67,0.99,0.1,0.7,0.94,0.1,0.97,0.29,0.94,0.96,0.75,0.1,0.96,1.07,1.02,0.94,0.98,0.95,0.1,0.15,0.97,0.97,0.99,0.98,1.01,0.96,0.98,0.96,0.98,0.99,0.97,0.99,0.96,0.99,0.1,0.99,0.97,1.01,0.98,0.98,0.97,1.02,0.95,0.99,0.94,0.96,0.97,0.99,0.1,0.84,0.97,1.01,0.99,0.99,0.96,0.99,1.06,0.99,0.96,0.99,1.06,0.96,0.83,0.69,0.98,0.96,1.02,0.91,0.94,1.02,0.45,0.99,0.1,0.85,0.99,0.1,0.95,0.23,1.03,0.82,0.96,1.13,0.93,1.09,1.04,0.93,0.96,0.98,0.1,0.25,0.98,0.98,0.98,0.98,0.99,0.92,0.99,0.96,0.99,1.02,0.99,0.98,0.1,0.95,0.98,1.04,0.99,1.05,1.09,1.05,1.02,0.97,1.01,0.95,0.99,1.01,1.02,0.92,0.8,1.01,1.01,0.97,1.06,0.97,0.97,1.05,0.96,1.04,1.02,0.97,1.03,0.99,0.8,0.65,0.96,1.02,0.95,0.93,0.99,0.71,0.97,0.1,1.33,0.99,0.99,0.22,0.85,0.1,1.04,0.91,1.07,1.07,1.01,1.3,0.99,1.04,0.99,0.63,0.5,1.02,0.98,0.98,1.03,0.92,1.02,0.98,0.99,1.02,1.05,1.02,1.02,1.01,0.1]\n",
    "}\n",
    "dept_adj = pd.DataFrame(dept_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f4d101e-53c8-4cf1-a350-48c86d371f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svd(num_folds_start,num_folds):\n",
    "    \"\"\"Train the model using SVD for smoothing.\"\"\"\n",
    "    start_time = time()\n",
    "    #num_folds = 10\n",
    "    \n",
    "    for i in range(num_folds_start,num_folds):\n",
    "        fold_start = time()\n",
    "        logging.info(f\"Processing fold {i+1}/{num_folds}\")\n",
    "        test_pred = pd.DataFrame()\n",
    "\n",
    "        # Read data for current fold\n",
    "        train = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/train.csv')\n",
    "        test = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/test.csv')\n",
    "        \n",
    "                # Convert dates to datetime\n",
    "        train['Date'] = pd.to_datetime(train['Date'])\n",
    "        test['Date'] = pd.to_datetime(test['Date'])\n",
    "\n",
    "        # Extract week, year, and other useful features\n",
    "        train['Wk'] = train['Date'].dt.isocalendar().week\n",
    "        train['Yr'] = train['Date'].dt.year\n",
    "        test['Wk'] = test['Date'].dt.isocalendar().week\n",
    "        test['Yr'] = test['Date'].dt.year\n",
    "        \n",
    "        train = add_holiday_flags(train)\n",
    "        test = add_holiday_flags(test)\n",
    "        \n",
    "        # Sort by Store, Dept, and Date for lag feature creation\n",
    "        train = train.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "        # Create lagged features\n",
    "        train['Lag_1'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "        train['Lag_2'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
    "        train['Rolling_Mean_4'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(4).mean())\n",
    "        train.dropna(inplace=True)  # Drop rows with NaN lagged features\n",
    "        \n",
    "            \n",
    "        departments = train['Dept'].unique()\n",
    "        logging.info(f\"Processing {len(departments)} departments for fold {i+1}\")\n",
    "        \n",
    "        # Process each department\n",
    "        for dept_idx, department in enumerate(departments, 1):\n",
    "            # if dept_idx % 5 == 0:  # Log every 5th department\n",
    "            #     logging.info(f\"  Progress: {dept_idx}/{len(departments)} departments\")\n",
    "                \n",
    "            filtered_train = train[train['Dept'] == department]\n",
    "            selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "\n",
    "            # Create pivot table and perform SVD\n",
    "            X_pivot = selected_columns.pivot(index='Date', columns='Store', \n",
    "                                          values='Weekly_Sales').fillna(0)        \n",
    "            X_matrix = X_pivot.values\n",
    "            date_mean = X_matrix.mean(axis=1, keepdims=True)\n",
    "            X_centered = X_matrix - date_mean\n",
    "            X_centered\n",
    "\n",
    "            # SVD computation\n",
    "            U, D, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "            n_comp = 8\n",
    "            D_tilda = np.zeros_like(D)\n",
    "            D_tilda[:n_comp] = D[:n_comp]\n",
    "            X_tilda = U[:, :n_comp] @ np.diag(D_tilda[:n_comp]) @ Vt[:n_comp, :]\n",
    "            X_smoothed = X_tilda + date_mean\n",
    "          \n",
    "\n",
    "            # Convert back to DataFrame\n",
    "            X_smoothed_df = pd.DataFrame(X_smoothed, index=X_pivot.index,\n",
    "                                       columns=X_pivot.columns).reset_index()\n",
    "            \n",
    "            # X_original_format = X_smoothed_df.melt(id_vars=['Store'], \n",
    "            #                                      var_name='Date', \n",
    "            #                                      value_name='Weekly_Sales')\n",
    "\n",
    "            X_original_format = X_smoothed_df.melt(\n",
    "                                                 id_vars=['Date'],\n",
    "                                                 var_name='Store',\n",
    "                                                 value_name='Weekly_Sales')\n",
    "            X_original_format['Date'] = pd.to_datetime(X_original_format['Date'])\n",
    "            X_original_format['Store'] = X_original_format['Store'].astype('int64')\n",
    "            \n",
    "            null_values = X_original_format.isnull().sum()\n",
    "            \n",
    "            X_original_format['Dept'] = department\n",
    "            X_original_format = X_original_format.sort_values(\n",
    "                by=['Store', 'Date']).reset_index(drop=True)\n",
    "            X_original_format = X_original_format[['Store','Date','Weekly_Sales','Dept']]\n",
    "            \n",
    "            \n",
    "\n",
    "            # Prepare train-test pairs\n",
    "            train_pairs = X_original_format[['Store', 'Dept']].drop_duplicates(\n",
    "                ignore_index=True)\n",
    "            test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "            unique_pairs = pd.merge(train_pairs, test_pairs, \n",
    "                                  how='inner', on=['Store', 'Dept'])\n",
    "            \n",
    "            # Process training data\n",
    "            train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "            train_split = preprocess(train_split)\n",
    "            \n",
    "            # Create model matrices\n",
    "            X = patsy.dmatrix('Weekly_Sales + Store + Dept  + IsHoliday+ Yr + Wk + Is_Christmas',\n",
    "                              data = train_split,\n",
    "                              return_type='dataframe')\n",
    "            \n",
    "            \n",
    "            \n",
    "            train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "            # Process test data\n",
    "            test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "            test_split = preprocess(test_split)\n",
    "            \n",
    "            \n",
    "            \n",
    "            X = patsy.dmatrix('Store + Dept + IsHoliday +Yr + Wk + Is_Christmas',\n",
    "                                data = test_split,\n",
    "                                return_type='dataframe')\n",
    "            \n",
    "            X['Date'] = test_split['Date']\n",
    "            test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "            keys = list(train_split)\n",
    "\n",
    "         \n",
    "\n",
    "            # Train and predict for each store-department combination\n",
    "            for key in keys:\n",
    "                X_train = train_split[key]\n",
    "                \n",
    "                # blend sample size to match expectation of the target formula - did not work\n",
    "                # row_to_repeat =  X_train[X_train['IsHoliday'] == 1]\n",
    "                # repeated_row_df = pd.concat([row_to_repeat] * 4, ignore_index=True)\n",
    "                # X_train = pd.concat([X_train, repeated_row_df], ignore_index=True)\n",
    "\n",
    "                \n",
    "                X_test = test_split[key]\n",
    "\n",
    "                #X_train['Intercept'] = 1.0\n",
    "                #print(X_train.columns)\n",
    "                #X_test['Intercept'] = 1.0\n",
    "\n",
    "                X_test_copy = X_test.copy()\n",
    "                X_train_copy = X_train.copy()\n",
    "\n",
    "                corr_mtx = X_train.corr()[['Weekly_Sales']]\n",
    "\n",
    "                ## variable selection seem to only improve the performance of the first week (no real week and year info to use)\n",
    "                if i+1 ==1:\n",
    "                    cor_threshold = 0.16\n",
    "                else:\n",
    "                    cor_threshold = 0\n",
    "                    \n",
    "                cols_to_keep = ['Intercept'] + corr_mtx[abs(corr_mtx['Weekly_Sales'])>cor_threshold].index.tolist()\n",
    "                #print(len(cols_to_keep))\n",
    "                if len(cols_to_keep) ==1:\n",
    "                    #bascially no model for cases where no good predictor identified\n",
    "                    cols_to_keep = X_train.columns.tolist()\n",
    "                #print(f'for key = {key}; high var columns- {cols_to_keep}')\n",
    "\n",
    "                \n",
    "                Y = X_train_copy['Weekly_Sales'] #.clip( upper=X_train['Weekly_Sales'].quantile(wzn_perc))\n",
    "                #Y = X_train['Weekly_Sales'].clip( upper=wzn_perc)\n",
    "                X_train = X_train[[col for col in cols_to_keep if col not in ['Weekly_Sales','Store', 'Dept','IsHoliday']]]\n",
    "                X_test = X_test[[col for col in cols_to_keep if col not in ['Weekly_Sales','Store', 'Dept','IsHoliday']]]\n",
    " \n",
    "\n",
    "                #X_train = X_train.drop(['Weekly_Sales','Store', 'Dept','IsHoliday'], axis=1)\n",
    "\n",
    "                cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "                X_train = X_train.drop(columns=cols_to_drop)\n",
    "                X_test = X_test.drop(columns=cols_to_drop)\n",
    "                cols_to_drop = []\n",
    "                for j in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "                    col_name = X_train.columns[j]\n",
    "                    # Extract the current column and all previous columns\n",
    "                    tmp_Y = X_train.iloc[:, j].values\n",
    "                    tmp_X = X_train.iloc[:, :j].values\n",
    "\n",
    "                    coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "                    if np.sum(residuals) < 1e-16:\n",
    "                            cols_to_drop.append(col_name)\n",
    "                #print(cols_to_drop)\n",
    "                X_train = X_train.drop(columns=cols_to_drop)\n",
    "                X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "                wzn_perc = 0.999\n",
    "                Y = Y.clip(upper=Y.quantile(wzn_perc))\n",
    "                model = sm.OLS(Y, X_train).fit()\n",
    "                mycoef = model.params.fillna(0)\n",
    "\n",
    "                # Make predictions\n",
    "                tmp_pred = X_test_copy[['Store', 'Dept', 'Date', 'IsHoliday']]\n",
    "                X_test = X_test.drop(['Store', 'Dept', 'Date', 'IsHoliday'], axis=1, errors='ignore')\n",
    "\n",
    "                tmp_pred['Weekly_Pred'] =  np.dot(X_test, mycoef)\n",
    "                # if i+1 == 5:\n",
    "                #     tmp_pred['Weekly_Pred'] = np.where((  ((tmp_pred['Store'] == 35) | (tmp_pred['Store']== 10)) &\n",
    "                #                                          ((tmp_pred['Date']== pd.to_datetime('2011-11-25'))) & \n",
    "                #                                          (tmp_pred['Dept']==72)), (640000), tmp_pred['Weekly_Pred'])\n",
    "\n",
    "\n",
    "                \n",
    "                test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "            test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "\n",
    "            test_pred['Wk'] = test_pred['Date'].dt.isocalendar().week\n",
    "            test_pred['Yr'] = test_pred['Date'].dt.year\n",
    "\n",
    "            test_pred = add_holiday_flags(test_pred)            \n",
    "            test_pred.to_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv', index=False)\n",
    "            \n",
    "\n",
    "                # Fit Gradient Boosting model \n",
    "                # model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "                # model.fit(X_train, Y)\n",
    "                \n",
    "        fold_time = time() - fold_start\n",
    "        logging.info(f\"Completed fold {i+1} in {fold_time:.1f} seconds\")\n",
    "        print(f\"Completed fold {i+1} in {fold_time:.1f} seconds\")\n",
    "    \n",
    "    total_time = time() - start_time\n",
    "    logging.info(f\"Completed all folds in {total_time:.1f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c01ed455-fdd4-4ab2-9fe8-84e73dad93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(num_folds_start,num_folds, dept_adj= dept_adj):\n",
    "    for i in range(num_folds_start,num_folds):\n",
    "        test_pred = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv')\n",
    "        \n",
    "        dept_adj_temp = dept_adj[dept_adj['folder']==i+1]\n",
    "        test_pred = pd.merge(test_pred,dept_adj_temp[['Dept','adj']], on = 'Dept', how = 'left')\n",
    "        test_pred['adj'].fillna(1.0, inplace=True)\n",
    "        \n",
    "        test_pred['Weekly_Pred'] = test_pred['Weekly_Pred']*test_pred['adj']\n",
    "        test_pred.to_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39d9e07d-6aea-44be-b8c8-0fabe00c0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(num_folds_start,num_folds):\n",
    "    \"\"\"Evaluate the model's predictions.\"\"\"\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    test_with_label = pd.read_csv(f'{DATA_DIR}/test_with_label.csv')\n",
    "    #num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds_start,num_folds):\n",
    "        # Read test data and predictions\n",
    "        test = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/test.csv')\n",
    "        test = test.merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "        test_pred = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv')\n",
    "        \n",
    "        # Merge and calculate weighted absolute error\n",
    "        new_test = test.merge(test_pred, on=['Date', 'Store', 'Dept'], how='left')\n",
    "        actuals = new_test['Weekly_Sales'].fillna(0)\n",
    "        preds = new_test['Weekly_Pred'].fillna(0)\n",
    "        weights = new_test['IsHoliday_x'].apply(lambda x: 5 if x else 1)\n",
    "\n",
    "        new_test['weights'] = weights \n",
    "        new_test['errors'] = actuals - preds\n",
    "        new_test['errors_abs'] = abs(actuals - preds)\n",
    "        new_test['errors_weighted'] = weights * abs(actuals - preds)\n",
    "        new_test.to_excel(f'{DATA_DIR}/fold_{i+1}/myvalidate.xlsx', index=False)\n",
    "        \n",
    "        wae_score = sum(weights * abs(actuals - preds)) / sum(weights)\n",
    "        wae.append(wae_score)\n",
    "        print(f\"Fold {i+1} WAE: {wae_score:.3f}\")\n",
    "        logging.info(f\"Fold {i+1} WAE: {wae_score:.3f}\")\n",
    "\n",
    "    avg_wae = sum(wae) / len(wae)\n",
    "    logging.info(f\"Average WAE across all folds: {avg_wae:.3f}\")\n",
    "    print(f\"Average WAE across all folds: {avg_wae:.3f}\")\n",
    "    \n",
    "    return wae\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c53a39a9-4857-4a82-99bd-0f9f1e5051bc",
   "metadata": {},
   "source": [
    "test_with_label = pd.read_csv(f'{DATA_DIR}/test_with_label.csv')\n",
    "test_with_label['Date'] = pd.to_datetime(test_with_label['Date'])\n",
    "\n",
    "test_with_label['Wk'] = test_with_label['Date'].dt.isocalendar().week\n",
    "test_with_label['Yr'] = test_with_label['Date'].dt.year\n",
    "test_with_label = preprocess(test_with_label)\n",
    "test_with_label = add_holiday_flags(test_with_label)\n",
    "test_with_label.to_excel(f'{DATA_DIR}/test_with_label_processed.xlsx')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5921f6c6-281d-4139-9239-07ccf423b574",
   "metadata": {},
   "source": [
    "# all training data from folder 10\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "\n",
    "train['Wk'] = train['Date'].dt.isocalendar().week\n",
    "train['Yr'] = train['Date'].dt.year\n",
    "train = preprocess(train)\n",
    "train = add_holiday_flags(train)\n",
    "train.to_excel(f'{DATA_DIR}/train_processed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfff2ca4-32be-4469-b000-4e7fc3748aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 1 in 38.1 seconds\n",
      "Completed fold 2 in 80.3 seconds\n",
      "Completed fold 3 in 77.5 seconds\n",
      "Completed fold 4 in 82.8 seconds\n",
      "Completed fold 5 in 86.2 seconds\n",
      "Completed fold 6 in 84.6 seconds\n",
      "Completed fold 7 in 84.6 seconds\n",
      "Completed fold 8 in 90.9 seconds\n",
      "Completed fold 9 in 87.9 seconds\n",
      "Completed fold 10 in 87.2 seconds\n",
      "Fold 1 WAE: 1782.592\n",
      "Fold 2 WAE: 1380.762\n",
      "Fold 3 WAE: 1352.247\n",
      "Fold 4 WAE: 1466.466\n",
      "Fold 5 WAE: 2171.280\n",
      "Fold 6 WAE: 1544.769\n",
      "Fold 7 WAE: 1729.611\n",
      "Fold 8 WAE: 1395.244\n",
      "Fold 9 WAE: 1379.188\n",
      "Fold 10 WAE: 1356.068\n",
      "Average WAE across all folds: 1555.823\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the Walmart price prediction model.\"\"\"\n",
    "    start_time = time()\n",
    "    logging.info(\"Starting Walmart Price Prediction\")\n",
    "    \n",
    "    logging.info(\"Training models with SVD smoothing...\")\n",
    "    train_svd(0,10)\n",
    "\n",
    "    logging.info(\"Post-processing...\")\n",
    "    post_processing(0,10)\n",
    "    \n",
    "    logging.info(\"Evaluating predictions...\")\n",
    "    wae = evaluate(0,10)\n",
    "    \n",
    "    total_time = time() - start_time\n",
    "    logging.info(f\"Processing complete! Total time: {total_time:.1f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef004740-6fbf-4025-abd4-64fa47814689",
   "metadata": {},
   "source": [
    "~~~~ benchmark ~~~~\n",
    "16:06:25 - Fold 1 WAE: 2323.037  -> improve to 1919 with variable selection\n",
    "16:06:25 - Fold 2 WAE: 1474.572\n",
    "16:06:25 - Fold 3 WAE: 1445.018\n",
    "16:06:25 - Fold 4 WAE: 1577.913\n",
    "16:06:25 - Fold 5 WAE: 2311.773 -> apply SVD by store (comp = 8) 2296 -> post-processing: 2171.280\n",
    "16:06:25 - Fold 6 WAE: 1652.971\n",
    "16:06:25 - Fold 7 WAE: 1689.754\n",
    "16:06:25 - Fold 8 WAE: 1364.755\n",
    "16:06:25 - Fold 9 WAE: 1356.631\n",
    "16:06:26 - Fold 10 WAE: 1338.518\n",
    "16:06:26 - Average WAE across all folds: 1653.494\n",
    "16:06:26 - Processing complete! Total time: 777.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8c3aa-80cf-471c-b3bf-b8b81d9cb630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
