{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqvqeu/hkQ6f8uOV6+Jvin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelal-Eizaldeen/project2_Walmart_Price_Prediction/blob/main/project3_3113_zelalae2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZjEGQz1WbSyc"
      },
      "outputs": [],
      "source": [
        "# To remove punctuation and numbers\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "#Vis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#ML\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Import BeautifulSoup for Removing HTML Markup\n",
        "from bs4 import BeautifulSoup\n",
        "# A stop word list from the Python Natural Language Toolkit (NLTK)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "2OmWHTV6boOg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugrwOFKsbsrq",
        "outputId": "4d593ad6-37c8-478a-870b-aa18e6d1e2cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Goal\n",
        "Your goal is to achieve an AUC score of at least 0.986 across all five test data splits."
      ],
      "metadata": {
        "id": "_E4QUsnoemsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources:\n",
        "- Kaggle: https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
        "- What we have tried 1: https://campuswire.com/c/GB46E5679/feed/785"
      ],
      "metadata": {
        "id": "i1NrBB1m4sAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR='/content/drive/MyDrive/MastersDegree/CS598 PSL/Assignments/Projects/Project3/Data/F24_Proj3_data'"
      ],
      "metadata": {
        "id": "skL4qA0Qbu6z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 By Zilal"
      ],
      "metadata": {
        "id": "BDAr_2f5lPK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n"
      ],
      "metadata": {
        "id": "VSgCURSMJPmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final model\n",
        "def train_logistic(X_train, y_train):\n",
        "    model = LogisticRegression(\n",
        "        penalty='elasticnet',\n",
        "        solver='saga',\n",
        "        l1_ratio=0, #means just Ridge Regression\n",
        "        C=5\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_hw87BwheY3s"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Prediction\n",
        "def predict(X_test):\n",
        "    X_test = test.drop(columns=['id', 'review'])\n",
        "    # Predict probabilities for the test data\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for positive sentiment\n",
        "    return y_pred_proba\n"
      ],
      "metadata": {
        "id": "2QZl2AUv44FL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_splits=5\n",
        "for i in range(num_splits):\n",
        "  train = pd.read_csv(f\"{DATA_DIR}/split_{i+1}/train.csv\")\n",
        "  test = pd.read_csv(f\"{DATA_DIR}/split_{i+1}/test.csv\")\n",
        "\n",
        "\n",
        "  # Separate features and target\n",
        "  X_train = train.drop(columns=['id', 'sentiment', 'review'])\n",
        "  y_train = train['sentiment']\n",
        "  X_test = test.drop(columns=['id', 'review'])\n",
        "\n",
        "\n",
        "  # Logistic regression without penalty as baseline\n",
        "  model = train_logistic(X_train, y_train)\n",
        "  y_pred_proba = predict(X_test)\n",
        "  auc_baseline = roc_auc_score(y_test, y_pred_proba)\n",
        "  print(f\"Baseline Logistic Regression AUC in split {i+1}: {auc_baseline:.3f}\")\n",
        "\n",
        "  submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'prob': y_pred_proba\n",
        "  })\n",
        "\n",
        "  submission.to_csv(f'{DATA_DIR}/split_{i+1}/mysubmission.csv', index=False)\n",
        "\n",
        "    # Calculate AUC on the test data\n",
        "  y_test = pd.read_csv(f'{DATA_DIR}/split_{i+1}/test_y.csv')['sentiment']\n",
        "\n",
        "  test_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "  print(f\"Best AUC Score on Test Data: in split {i+1}\", test_auc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCihAbT1ZgSr",
        "outputId": "87460cd8-6620-42c3-9890-a106634a7301"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Logistic Regression AUC in split 1: 0.499\n",
            "Best AUC Score on Test Data: in split 1 0.9871402088013582\n",
            "Baseline Logistic Regression AUC in split 2: 0.501\n",
            "Best AUC Score on Test Data: in split 2 0.9867615833393729\n",
            "Baseline Logistic Regression AUC in split 3: 0.506\n",
            "Best AUC Score on Test Data: in split 3 0.9864233090877216\n",
            "Baseline Logistic Regression AUC in split 4: 0.504\n",
            "Best AUC Score on Test Data: in split 4 0.986954014050569\n",
            "Baseline Logistic Regression AUC in split 5: 0.503\n",
            "Best AUC Score on Test Data: in split 5 0.9863475507543306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Part 2 By Jianci and Messay**"
      ],
      "metadata": {
        "id": "vJYUU03hk2Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Libraries And Packages\n"
      ],
      "metadata": {
        "id": "CgpfcsgOvdbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing HTML Markup: The BeautifulSoup Package\n",
        "!pip install BeautifulSoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eljbS0ztvgiO",
        "outputId": "56e3a5c6-d150-4d19-8948-5c171515d798"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning and Text Preprocessing\n",
        "- Removing HTML Markup: The BeautifulSoup Package\n",
        "- Frequently occurring words that don't carry much meaning. Such words are called \"stop words\"."
      ],
      "metadata": {
        "id": "fCgQO6n5fWKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')  # Download text data sets, including stop words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwfNIE8PwBsX",
        "outputId": "06919f11-1003-4ead-c877-8ae34b23934d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_words( raw_review ):\n",
        "    # Function to convert a raw review to a string of words\n",
        "    # The input is a single string (a raw movie review), and\n",
        "    # the output is a single string (a preprocessed movie review)\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review).get_text()\n",
        "    #\n",
        "    # 2. Remove non-letters\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
        "    #\n",
        "    # 3. Convert to lower case, split into individual words\n",
        "    words = letters_only.lower().split()\n",
        "    #\n",
        "    # 4. In Python, searching a set is much faster than searching\n",
        "    #   a list, so convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    #\n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 6. Join the words back into one string separated by space,\n",
        "    # and return the result.\n",
        "\n",
        "    return( \" \".join( meaningful_words ))"
      ],
      "metadata": {
        "id": "Bz5_tXbC9dHx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop through and clean all of the training set at once"
      ],
      "metadata": {
        "id": "AFF2aiS6-FHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of reviews based on the dataframe column size\n",
        "num_reviews = train[\"review\"].size\n",
        "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
        "# Initialize an empty list to hold the clean reviews\n",
        "clean_train_reviews = []\n",
        "for i in range( 0, num_reviews ):\n",
        "    # If the index is evenly divisible by 1000, print a message\n",
        "    if( (i+1)%1000 == 0 ):\n",
        "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
        "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtVRyM0O-Zf9",
        "outputId": "1a82bbe0-a03c-4c09-bf2d-244691820f1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and parsing the training set movie reviews...\n",
            "\n",
            "Review 1000 of 25000\n",
            "\n",
            "Review 2000 of 25000\n",
            "\n",
            "Review 3000 of 25000\n",
            "\n",
            "Review 4000 of 25000\n",
            "\n",
            "Review 5000 of 25000\n",
            "\n",
            "Review 6000 of 25000\n",
            "\n",
            "Review 7000 of 25000\n",
            "\n",
            "Review 8000 of 25000\n",
            "\n",
            "Review 9000 of 25000\n",
            "\n",
            "Review 10000 of 25000\n",
            "\n",
            "Review 11000 of 25000\n",
            "\n",
            "Review 12000 of 25000\n",
            "\n",
            "Review 13000 of 25000\n",
            "\n",
            "Review 14000 of 25000\n",
            "\n",
            "Review 15000 of 25000\n",
            "\n",
            "Review 16000 of 25000\n",
            "\n",
            "Review 17000 of 25000\n",
            "\n",
            "Review 18000 of 25000\n",
            "\n",
            "Review 19000 of 25000\n",
            "\n",
            "Review 20000 of 25000\n",
            "\n",
            "Review 21000 of 25000\n",
            "\n",
            "Review 22000 of 25000\n",
            "\n",
            "Review 23000 of 25000\n",
            "\n",
            "Review 24000 of 25000\n",
            "\n",
            "Review 25000 of 25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating the bag of words...\\n\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",\n",
        "                             tokenizer = None,\n",
        "                             preprocessor = None,\n",
        "                             stop_words = None,\n",
        "                             max_features = 5000,\n",
        "                             ngram_range=(1, 4),            # Use 1- to 4-grams\n",
        "                             min_df=0.001,                       # Minimum term frequency\n",
        "                             max_df=0.5,                      # Maximum document frequency\n",
        "                             token_pattern=r\"\\b[\\w+|']+\\b\") # Use word tokenizer: See Ethan's comment below\n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of\n",
        "# strings.\n",
        "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an\n",
        "# array\n",
        "train_data_features = train_data_features.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD0uvMj9QjKD",
        "outputId": "11581330-6b0f-4fb4-a026-58b89fe77326"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the bag of words...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the words in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUg9eiv5FLNR",
        "outputId": "b292ecc0-d435-486f-919c-792f462335c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abandoned' 'abilities' 'ability' ... 'zombie' 'zombies' 'zone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum up the counts of each vocabulary word\n",
        "dist = np.sum(train_data_features, axis=0)\n",
        "\n",
        "# For each, print the vocabulary word and the number of times it\n",
        "# appears in the training set\n",
        "for tag, count in zip(vocab, dist):\n",
        "    print(count, tag)"
      ],
      "metadata": {
        "id": "OlUE6SGJSI4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the random forest...\")\n",
        "\n",
        "# Initialize a Random Forest classifier with 100 trees\n",
        "forest = RandomForestClassifier(n_estimators = 100)\n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as\n",
        "# features and the sentiment labels as the response variable\n",
        "#\n",
        "# This may take a few minutes to run\n",
        "forest = forest.fit( train_data_features, train[\"sentiment\"] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x02rzfuuAbsj",
        "outputId": "cc468684-c624-4f13-db58-488b576904cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the random forest...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list and append the clean reviews one by one\n",
        "num_reviews = len(test[\"review\"])\n",
        "clean_test_reviews = []\n",
        "\n",
        "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
        "for i in range(0,num_reviews):\n",
        "    if( (i+1) % 1000 == 0 ):\n",
        "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
        "    clean_review = review_to_words( test[\"review\"][i] )\n",
        "    clean_test_reviews.append( clean_review )\n",
        "\n",
        "# Get a bag of words for the test set, and convert to a numpy array\n",
        "test_data_features = vectorizer.transform(clean_test_reviews)\n",
        "test_data_features = test_data_features.toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbxV_LX-CNeh",
        "outputId": "a9009220-dcc8-4602-eb30-2724bca7430d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and parsing the test set movie reviews...\n",
            "\n",
            "Review 1000 of 25000\n",
            "\n",
            "Review 2000 of 25000\n",
            "\n",
            "Review 3000 of 25000\n",
            "\n",
            "Review 4000 of 25000\n",
            "\n",
            "Review 5000 of 25000\n",
            "\n",
            "Review 6000 of 25000\n",
            "\n",
            "Review 7000 of 25000\n",
            "\n",
            "Review 8000 of 25000\n",
            "\n",
            "Review 9000 of 25000\n",
            "\n",
            "Review 10000 of 25000\n",
            "\n",
            "Review 11000 of 25000\n",
            "\n",
            "Review 12000 of 25000\n",
            "\n",
            "Review 13000 of 25000\n",
            "\n",
            "Review 14000 of 25000\n",
            "\n",
            "Review 15000 of 25000\n",
            "\n",
            "Review 16000 of 25000\n",
            "\n",
            "Review 17000 of 25000\n",
            "\n",
            "Review 18000 of 25000\n",
            "\n",
            "Review 19000 of 25000\n",
            "\n",
            "Review 20000 of 25000\n",
            "\n",
            "Review 21000 of 25000\n",
            "\n",
            "Review 22000 of 25000\n",
            "\n",
            "Review 23000 of 25000\n",
            "\n",
            "Review 24000 of 25000\n",
            "\n",
            "Review 25000 of 25000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}