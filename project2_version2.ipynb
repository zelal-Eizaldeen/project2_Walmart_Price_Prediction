{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLzH+vCn1Fu8Ckqv4u55VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelal-Eizaldeen/project2_Walmart_Price_Prediction/blob/main/project2_version2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Ims9nWLMjdQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import statsmodels.api as sm\n",
        "import patsy\n",
        "\n",
        "from datetime import timedelta\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "What We have tried 1: https://liangfgithub.github.io/Proj/F24_Proj2_hints_1_Python.html\n",
        "\n",
        "What We have tried 2: https://liangfgithub.github.io/Proj/F24_Proj2_hints_2_Python.html\n",
        "\n",
        "What we have learned 3: https://campuswire.com/c/GB46E5679/feed/457\n",
        "\n",
        "performance target: 1580 or less; 1pt: above 1680"
      ],
      "metadata": {
        "id": "URquub-NOWxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "TjMOFMZZM5q6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ZwgDmDbtNGq8",
        "outputId": "e7becc6b-2940-4bab-f355-a14a83535984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Replace with your path to the Proj2_Data folder on GoogleColab\n",
        "path_to_data='/content/drive/MyDrive/MastersDegree/CS598 PSL/Assignments/Projects/Project2/MyCode/Proj2_Data'\n",
        "DATA_DIR ='/content/drive/MyDrive/MastersDegree/CS598 PSL/Assignments/Projects/Project2/MyCode/Proj2_Data'"
      ],
      "metadata": {
        "id": "KAHlggZ1NNjQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "def preprocess(data):\n",
        "    \"\"\"Preprocess the data by handling missing values and creating time-based features.\"\"\"\n",
        "    data.fillna(0, inplace=True)\n",
        "    tmp = pd.to_datetime(data['Date'])\n",
        "    data['Wk'] = tmp.dt.isocalendar().week\n",
        "    data['Yr'] = tmp.dt.year\n",
        "    data['Yr2'] = data.Yr ** 2\n",
        "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])\n",
        "    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
        "    return data"
      ],
      "metadata": {
        "id": "sp6l6BXmNag8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to mark holiday weeks\n",
        "def add_holiday_flags(df):\n",
        "    df['Is_SuperBowl'] = (df['Wk'] == 6).astype(int)\n",
        "    df['Is_Thanksgiving'] = (df['Wk'] == 47).astype(int)\n",
        "    df['Is_Christmas'] = (df['Wk'] == 52).astype(int)\n",
        "    return df"
      ],
      "metadata": {
        "id": "WG-lq-RoNc2c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svd():\n",
        "    \"\"\"Train the model using SVD for smoothing.\"\"\"\n",
        "    start_time = time()\n",
        "    num_folds = 10\n",
        "\n",
        "    for i in range(num_folds):\n",
        "        fold_start = time()\n",
        "        logging.info(f\"Processing fold {i+1}/{num_folds}\")\n",
        "        test_pred = pd.DataFrame()\n",
        "\n",
        "        # Read data for current fold\n",
        "        train = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/train.csv')\n",
        "        test = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/test.csv')\n",
        "\n",
        "                # Convert dates to datetime\n",
        "        train['Date'] = pd.to_datetime(train['Date'])\n",
        "        test['Date'] = pd.to_datetime(test['Date'])\n",
        "\n",
        "        # Extract week, year, and other useful features\n",
        "        train['Wk'] = train['Date'].dt.isocalendar().week\n",
        "        train['Yr'] = train['Date'].dt.year\n",
        "        test['Wk'] = test['Date'].dt.isocalendar().week\n",
        "        test['Yr'] = test['Date'].dt.year\n",
        "\n",
        "        train = add_holiday_flags(train)\n",
        "        test = add_holiday_flags(test)\n",
        "\n",
        "        # Sort by Store, Dept, and Date for lag feature creation\n",
        "        train = train.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        # Create lagged features\n",
        "        train['Lag_1'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
        "        train['Lag_2'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
        "        train['Rolling_Mean_4'] = train.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(4).mean())\n",
        "        train.dropna(inplace=True)  # Drop rows with NaN lagged features\n",
        "\n",
        "\n",
        "        departments = train['Dept'].unique()\n",
        "        logging.info(f\"Processing {len(departments)} departments for fold {i+1}\")\n",
        "\n",
        "        # Process each department\n",
        "        for dept_idx, department in enumerate(departments, 1):\n",
        "            if dept_idx % 5 == 0:  # Log every 5th department\n",
        "                logging.info(f\"  Progress: {dept_idx}/{len(departments)} departments\")\n",
        "\n",
        "            filtered_train = train[train['Dept'] == department]\n",
        "            selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
        "\n",
        "            # Create pivot table and perform SVD\n",
        "            X_pivot = selected_columns.pivot(index='Store', columns='Date',\n",
        "                                          values='Weekly_Sales').fillna(0)\n",
        "            X_matrix = X_pivot.values\n",
        "            store_mean = X_matrix.mean(axis=1, keepdims=True)\n",
        "            X_centered = X_matrix - store_mean\n",
        "\n",
        "            # SVD computation\n",
        "            U, D, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
        "            n_comp = 8\n",
        "            D_tilda = np.zeros_like(D)\n",
        "            D_tilda[:n_comp] = D[:n_comp]\n",
        "            X_tilda = U[:, :n_comp] @ np.diag(D_tilda[:n_comp]) @ Vt[:n_comp, :]\n",
        "            X_smoothed = X_tilda + store_mean\n",
        "\n",
        "\n",
        "            # Convert back to DataFrame\n",
        "            X_smoothed_df = pd.DataFrame(X_smoothed, index=X_pivot.index,\n",
        "                                       columns=X_pivot.columns).reset_index()\n",
        "\n",
        "            X_original_format = X_smoothed_df.melt(id_vars=['Store'],\n",
        "                                                 var_name='Date',\n",
        "                                                 value_name='Weekly_Sales')\n",
        "\n",
        "            null_values = X_original_format.isnull().sum()\n",
        "\n",
        "            X_original_format['Dept'] = department\n",
        "            X_original_format = X_original_format.sort_values(\n",
        "                by=['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "            # Prepare train-test pairs\n",
        "            train_pairs = X_original_format[['Store', 'Dept']].drop_duplicates(\n",
        "                ignore_index=True)\n",
        "            test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
        "            unique_pairs = pd.merge(train_pairs, test_pairs,\n",
        "                                  how='inner', on=['Store', 'Dept'])\n",
        "\n",
        "            # Process training data\n",
        "            train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
        "            train_split = preprocess(train_split)\n",
        "\n",
        "            # Create model matrices\n",
        "            X = patsy.dmatrix('Weekly_Sales + Store + Dept  + IsHoliday+ Yr + Yr2 + Wk',\n",
        "                              data = train_split,\n",
        "                              return_type='dataframe')\n",
        "\n",
        "\n",
        "\n",
        "            train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
        "\n",
        "            # Process test data\n",
        "            test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
        "            test_split = preprocess(test_split)\n",
        "\n",
        "\n",
        "\n",
        "            X = patsy.dmatrix('Store + Dept + IsHoliday +Yr + Yr2 + Wk',\n",
        "                                data = test_split,\n",
        "                                return_type='dataframe')\n",
        "\n",
        "            X['Date'] = test_split['Date']\n",
        "            test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
        "            keys = list(train_split)\n",
        "\n",
        "\n",
        "\n",
        "            # Train and predict for each store-department combination\n",
        "            for key in keys:\n",
        "                X_train = train_split[key]\n",
        "                X_test = test_split[key]\n",
        "\n",
        "                Y = X_train['Weekly_Sales']\n",
        "                X_train = X_train.drop(['Weekly_Sales','Store', 'Dept','IsHoliday'], axis=1)\n",
        "\n",
        "                cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
        "                X_train = X_train.drop(columns=cols_to_drop)\n",
        "                X_test = X_test.drop(columns=cols_to_drop)\n",
        "                cols_to_drop = []\n",
        "                for j in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
        "                    col_name = X_train.columns[j]\n",
        "                    # Extract the current column and all previous columns\n",
        "                    tmp_Y = X_train.iloc[:, j].values\n",
        "                    tmp_X = X_train.iloc[:, :j].values\n",
        "\n",
        "                    coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
        "                    if np.sum(residuals) < 1e-16:\n",
        "                            cols_to_drop.append(col_name)\n",
        "                X_train = X_train.drop(columns=cols_to_drop)\n",
        "                X_test = X_test.drop(columns=cols_to_drop)\n",
        "\n",
        "                model = sm.OLS(Y, X_train).fit()\n",
        "                mycoef = model.params.fillna(0)\n",
        "\n",
        "                # Make predictions\n",
        "                tmp_pred = X_test[['Store', 'Dept', 'Date', 'IsHoliday']]\n",
        "                X_test = X_test.drop(['Store', 'Dept', 'Date', 'IsHoliday'], axis=1, errors='ignore')\n",
        "\n",
        "                tmp_pred['Weekly_Pred'] =  np.dot(X_test, mycoef)\n",
        "                test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
        "\n",
        "            test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
        "            test_pred.to_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv', index=False)\n",
        "\n",
        "\n",
        "                # Fit Gradient Boosting model\n",
        "                # model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
        "                # model.fit(X_train, Y)\n",
        "\n",
        "        fold_time = time() - fold_start\n",
        "        logging.info(f\"Completed fold {i+1} in {fold_time:.1f} seconds\")\n",
        "\n",
        "    total_time = time() - start_time\n",
        "    logging.info(f\"Completed all folds in {total_time:.1f} seconds\")"
      ],
      "metadata": {
        "id": "eZZpL0oNNgzp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    \"\"\"Evaluate the model's predictions.\"\"\"\n",
        "    logging.info(\"Starting evaluation...\")\n",
        "    test_with_label = pd.read_csv(f'{DATA_DIR}/test_with_label.csv')\n",
        "    num_folds = 10\n",
        "    wae = []\n",
        "\n",
        "    for i in range(num_folds):\n",
        "        # Read test data and predictions\n",
        "        test = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/test.csv')\n",
        "        test = test.merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
        "        test_pred = pd.read_csv(f'{DATA_DIR}/fold_{i+1}/mypred.csv')\n",
        "\n",
        "        # Merge and calculate weighted absolute error\n",
        "        new_test = test.merge(test_pred, on=['Date', 'Store', 'Dept'], how='left')\n",
        "        actuals = new_test['Weekly_Sales'].fillna(0)\n",
        "        preds = new_test['Weekly_Pred'].fillna(0)\n",
        "        weights = new_test['IsHoliday_x'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "        wae_score = sum(weights * abs(actuals - preds)) / sum(weights)\n",
        "        wae.append(wae_score)\n",
        "        logging.info(f\"Fold {i+1} WAE: {wae_score:.3f}\")\n",
        "\n",
        "    avg_wae = sum(wae) / len(wae)\n",
        "    logging.info(f\"Average WAE across all folds: {avg_wae:.3f}\")\n",
        "\n",
        "    return wae"
      ],
      "metadata": {
        "id": "D9cHXQ-3NlRJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the Walmart price prediction model.\"\"\"\n",
        "    start_time = time()\n",
        "    logging.info(\"Starting Walmart Price Prediction\")\n",
        "\n",
        "    logging.info(\"Training models with SVD smoothing...\")\n",
        "    train_svd()\n",
        "\n",
        "    logging.info(\"Evaluating predictions...\")\n",
        "    wae = evaluate()\n",
        "\n",
        "    total_time = time() - start_time\n",
        "    logging.info(f\"Processing complete! Total time: {total_time:.1f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "eJIhK4r8N3cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}