# -*- coding: utf-8 -*-
"""Project2_Walmart_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15q51uwr8TSYTeNtJzR1mLWB8fdpaeCnS

##PROJECT 2

References:
- What We have tried 1: https://liangfgithub.github.io/Proj/F24_Proj2_hints_1_Python.html
"""

import pandas as pd
from datetime import datetime, timedelta
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ML
import statsmodels.api as sm
import patsy



# from google.colab import drive
# drive.mount('/content/drive')

##Replace with your path to the Proj2_Data folder on GoogleColab
path_to_data='Proj2_Data'

"""## **Data Exploration**"""

train = pd.read_csv(f'{path_to_data}/fold_1/train.csv')
test = pd.read_csv(f'{path_to_data}/fold_1/test.csv')

data_train = train.copy()
data_test = test.copy()

# Filling missing values
data_train.isna().sum()[data_train.isna().sum() > 0].sort_values(ascending=False)
data_test.isna().sum()[data_test.isna().sum() > 0].sort_values(ascending=False)
data_train.fillna(0, inplace = True)
data_test.fillna(0, inplace = True)

# Preprocessing the data
def preprocess(data):
    tmp = pd.to_datetime(data['Date'])
    data['Wk'] = tmp.dt.isocalendar().week
    data['Yr'] = tmp.dt.year
    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks
#    data['IsHoliday'] = data['IsHoliday'].apply(int)
    return data

train = pd.read_csv(f'{path_to_data}/fold_1/train.csv')
test = pd.read_csv(f'{path_to_data}/fold_1/test.csv')


# pre-allocate a pd to store the predictions
test_pred = pd.DataFrame()

train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)
test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)
unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])

train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')
train_split = preprocess(train_split)
X = patsy.dmatrix('Weekly_Sales + Store + Dept + Yr  + Wk',
                  data = train_split,
                  return_type='dataframe')

train_split = dict(tuple(X.groupby(['Store', 'Dept'])))


test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')
test_split = preprocess(test_split)
X = patsy.dmatrix('Store + Dept + Yr  + Wk', 
                    data = test_split, 
                    return_type='dataframe')
X['Date'] = test_split['Date']
test_split = dict(tuple(X.groupby(['Store', 'Dept'])))

keys = list(train_split)

for key in keys:
    X_train = train_split[key]
    X_test = test_split[key]
 
    Y = X_train['Weekly_Sales']
    X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)
    
    cols_to_drop = X_train.columns[(X_train == 0).all()]
    X_train = X_train.drop(columns=cols_to_drop)
    X_test = X_test.drop(columns=cols_to_drop)
 
    cols_to_drop = []
    for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward
        col_name = X_train.columns[i]
        # Extract the current column and all previous columns
        tmp_Y = X_train.iloc[:, i].values
        tmp_X = X_train.iloc[:, :i].values

        coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)
        if np.sum(residuals) < 1e-16:
                cols_to_drop.append(col_name)
            
    X_train = X_train.drop(columns=cols_to_drop)
    X_test = X_test.drop(columns=cols_to_drop)

    model = sm.OLS(Y, X_train).fit()
    mycoef = model.params.fillna(0)
    
    tmp_pred = X_test[['Store', 'Dept', 'Date']]
    X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)
    
    tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)
    test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)
    
test_pred['Weekly_Pred'].fillna(0, inplace=True)
# Write the output to CSV
test_pred.to_csv(f'{path_to_data}/fold_1/mypred.csv', index=False)